### CLIP

[paper link](https://arxiv.org/pdf/2103.00020)

> ìš©ì–´
>
> > transfer: ì‚¬ì „ í•™ìŠµëœ ì§€ì‹ì„ ìƒˆë¡œìš´ ì‘ì—…ì— ì ìš©í•˜ëŠ” ê²ƒ<br>
> > supervision (learning): ì§€ë„ (í•™ìŠµ)<br>
> > pre-training: ëª¨ë¸ì´ íŠ¹ì •í•œ ì‘ì—…ì— ì´ˆì ì„ ë§ì¶”ê¸° ì „ì—, ëŒ€ê·œëª¨ì˜ ì¼ë°˜ì ì¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ì ì¸ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ” -

0. Abstract

   > State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories

   ìµœì²¨ë‹¨ ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œì€ ë¯¸ë¦¬ ì •í•´ì§„ ê³ ì •ëœ ì§‘í•©ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡.

   - ì „í†µì ì¸ ì»´í“¨í„° ë¹„ì „ì€ íŠ¹ì • ì‘ì—…ì„ ìœ„í•´ í•™ìŠµëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©. ex) ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì€ ê³ ì–‘ì´ì™€ ê°œë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ê³ ì–‘ì´ì™€ ê°œ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•¨. ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ê³ ì •ëœ ì¹´í…Œê³ ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  íŠ¹ì • ë°ì´í„°ì…‹ì— íŠ¹í™”ë˜ì–´ìˆìŒ.

   - ë¯¸ë¦¬ ì •í•´ì§„ ê³ ì •ëœ ì§‘í•©ì˜ ì¹´í…Œê³ ë¦¬: [ê°•ì•„ì§€, ê³ ì–‘ì´, ìë™ì°¨ ...]

   - íŠ¹ì • ë°ì´í„°ì…‹ì— íŠ¹í™”: ë°ì´í„°ì…‹ì— í¬í•¨ëœ ê°ì²´ë“¤ë§Œ ì¸ì‹ ê°€ëŠ¥. ê·¸ ì™¸ ìƒˆë¡œì€ ê°ì²´ë¥¼ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•´ì•¼ í•¨.

   > this restrict form of supervision limits their generality and usability since additional labled data is needed to specify any other visual concept

   > ì´ëŸ¬í•œ restrict formì˜ supervisionì€ limitsí•œë‹¤. ê·¸ë“¤ì˜ generalityì™€ usabilityë¥¼. since ì¶”ê°€ì ì¸ labeled dataê°€ í•„ìš”ë˜ì–´ì§ˆ ë•Œ. ì–´ë– í•œ ë‹¤ë¥¸ visual conceptë¥¼.

   ì´ëŸ¬í•œ ì œí•œëœ í˜•ì‹ì˜ supervisionì€ ê·¸ë“¤ì˜ ì¼ë°˜í™”ì™€ ì‚¬ìš©ì„±ì„ ì œí•œí•œë‹¤. ì œí•œì ì¸ ë ˆì´ë¸”ëœ ë°ì´í„°ëŠ” ì–´ë– í•œ ë‹¤ë¥¸ ì‹œê°ì ì¸ ê°œë…ì„ íŠ¹ì •í•˜ëŠ”ë° í•„ìš”ë˜ì–´ì§€ê¸° ë•Œë¬¸ì—.

   - ì¦‰, ê¸°ì¡´ supervision leariningì˜ í•œê³„ë¥¼ ë§í•˜ê³  ìˆìŒ.

   - ê¸°ì¡´ ì§€ë„ í•™ìŠµ ë°©ì‹ì—ì„œëŠ” ëª¨ë¸ì´ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„œ, ê·¸ ì‘ì—…ì— í•´ë‹¹í•˜ëŠ” ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œëŠ”ë°, ìœ„ì—ì„œ ë§í–ˆë‹¤ì‹œí”¼, ë¯¸ë¦¬ ì •í•´ì§„ ê³ ì •ëœ ì§‘í•©ì˜ ì¹´í…Œê³ ë¦¬ ì™¸ì˜ ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ê°€í•˜ëŠ” ê²Œ í˜ë“¦. ë¼ë²¨ë§ ë‹¤ì‹œ í•´ì•¼í•˜ê³ , ë°ì´í„°ì…‹ë„ í•„ìš”í•˜ê³  í•´ì„œ.. ë¬¼ë¡  ê·¸ ê³¼ì •ì—ì„œ ì—„ì²­ë‚œ ì‹œê°„ê³¼ ëˆì´ ë“¤ì–´ê°.

   > learning directly from raw text about imageëŠ” a promising alternative which leverages a much broader source of supervision

   ì´ë¯¸ì§€ì™€ í•¨ê»˜ raw textë¥¼ ë°”ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ supervisionì˜ í›¨ì”¬ ë” ë„“ì€ sourceë¥¼ í™œìš©í•˜ëŠ” ê²ƒì— ìœ ë§í•œ ëŒ€ì²´ì´ë‹¤.

   ì¦‰, ì´ë¯¸ì§€ì™€ ê·¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…ì´ í•¨ê»˜ ì œê³µë˜ëŠ” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ í›¨ì”¬ ë” ë„“ì€ ë²”ìœ„ì˜ supervisonì„ í™œìš©í•˜ëŠ” ê²ƒì— ëŒ€í•œ ìœ ë§í•œ ëŒ€ì•ˆì„ì„ ì£¼ì¥.

   - ì¦‰, ì´ë¯¸ì§€ì— ë¼ë²¨ì„ ë¶™ì´ê³  í•™ìŠµì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤, ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ì˜ ìº¡ì…˜ ìŒì„ í•¨ê»˜ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ í›¨ì”¬ ì„±ëŠ¥ì´ ì¢‹ì„ ê±°ë‹¤ë¼ê³  ë§í•˜ëŠ” ê±´ê°€

   - ê¸°ì¡´ì—ëŠ” `{ê°•ì•„ì§€: [ê°•ì•„ì§€ ì´ë¯¸ì§€1, ê°•ì•„ì§€ ì´ë¯¸ì§€2 ... ê°•ì•„ì§€ ì´ë¯¸ì§€ n]}`ìœ¼ë¡œ í•™ìŠµì‹œì¼°ë‹¤ë©´, ì´ ë…¼ë¬¸ì—ì„œëŠ” `{ê°•ì•„ì§€ ì´ë¯¸ì§€1: "ë“¤íŒì„ ë›°ì–´ë‹¤ë‹ˆëŠ” ê°•ì•„ì§€", ê°•ì•„ì§€ ì´ë¯¸ì§€2: "ê°„ì‹ì„ ë¨¹ëŠ” ê°•ì•„ì§€" ... }`ì™€ ê°™ì´ ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ê²ƒ.

   - ì¦‰, ì´ë¯¸ì§€ì™€ í•¨ê»˜ raw textë¥¼ ë°”ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì´ ê¸°ì¡´ì˜ ì „í†µì ì¸ supervision learning ë°©ì‹ì— ë¹„í•´ ë” ìœ ë§í•œ ëŒ€ì•ˆì„ì„ ë‚˜íƒ€ëƒ„. -> í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í†µí•´ í›¨ì”¬ ë” ë„“ì€ ë²”ìœ„ì˜ visual concept ìì²´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŒ -> ëª¨ë¸ì˜ ë²”ìš©ì„± & í™•ì¥ì„±?ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ.

   > we demonstrate that _the simple pre-training task of predicting which caption goes with which image_ is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (img-text) pairs collected from the internet.

   > ë¬¸ì¥ ì™¤ì¼€ ê¸¸ì–´

   > ìš°ë¦¬ëŠ” (í•´ë‹¹ ë…¼ë¬¸ì—ì„œ) ì‹œì—°í•œë‹¤. ê°„ë‹¨í•œ pre-training taskì˜ ì˜ˆì¸¡ì´, ì–´ë– í•œ ìº¡ì…˜ì´ ì–´ìš¸ë¦¬ëŠ”ì§€ with ì–´ë– í•œ ì´ë¯¸ì§€ê°€ íš¨ê³¼ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ,ë°©ì‹ìœ¼ë¡œ SOTA ì´ë¯¸ì§€ í‘œí˜„ì„, í•™ìŠµí•˜ëŠ” ë°©ë²• / from scratch on 400 millionì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì´ ëª¨ì—¬ì§„ ë°ì´í„°ì…‹ì—ì„œ, ì¸í„°ë„·ìœ¼ë¡œë¶€í„°

   ìš°ë¦¬ëŠ” ì–´ë– í•œ ì´ë¯¸ì§€ì— ì–´ë– í•œ ìº¡ì…˜ì´ ì–´ìš¸ë¦¬ëŠ”ì§€ ì˜ˆì¸¡í•˜ëŠ” ê°„ë‹¨í•œ pre-trained taskê°€ SOTA ì´ë¯¸ì§€ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” íš¨ê³¼ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°©ë²•ì„ì„ ì‹œì—°í•œë‹¤. / from ì¸í„°ë„·ì—ì„œ 400 millionì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì´ collectedëœ ë°ì´í„°ì…‹ì„ ê°€ì ¸ì™€ì„œ.

   - ëŒ€ì¶© ì´ ë…¼ë¬¸ì—ì„œ `ìœ„ì—ì„œ ë§í•œ ê±° í…ŒìŠ¤íŠ¸ í•´ë´ì‹ ë”” ìƒê°ë³´ë‹¤ ì¢‹ë”ë¼` ë¼ëŠ” ê²ƒì„ ë§í•˜ê³  ì‹¶ì€ ê±° ê°™ìŒ.

   > After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks.

   > pre-training ì´í›„, ìì—°ì–´ëŠ” usedëœë‹¤. to reference learned visual concepts (or descrive new ones) enabling zero-shot transfer of the model to downstream tasts.

   pre-training ì´í›„, ìì—°ì–´ëŠ” í•™ìŠµëœ visual conceptsë¥¼ ì°¸ì¡°í•˜ê±°ë‚˜, ìƒˆë¡œìš´ ê²ƒì„ ì„¤ëª…í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª¨ë¸ì„ í•˜ìœ„ taskë¡œ zero-shot transferê°€ ê°€ëŠ¥í•˜ë„ë¡ ì‚¬ìš©ëœë‹¤.

   - `ìì—°ì–´ê°€ í•™ìŠµëœ ì‹œê°ì  ê°œë…ì„ ì°¸ì¡°í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤`ê°€ ë­˜ ëœ»í•˜ëŠ” ê±°ì§€... ì—¬ê¸°ì„œ ë§í•˜ëŠ” `refernece`ëŠ” ë­ ë‹¤ë¥¸ ëœ»ì´ ìˆëŠ” ê±´ê°€..

   - ì—¬íŠ¼, pre-training í•´ì„œ ë³´ë‹ˆê¹Œ zero-shot transferê°€ ë¼ì„œ ì—¬ëŸ¬ê°€ì§€ë¡œ í™œìš© ê°€ëŠ¥í•˜ë‹¤ê³  ë§í•˜ëŠ” ê±° ê°™ìŒ.

   > We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification

   > ìš°ë¦¬ëŠ” ì´ ì ‘ê·¼ performanceë¥¼ ì—°êµ¬í–ˆë‹¤. / 30ê°œ ì´ìƒì˜ ë‹¤ë¥¸, ì¡´ì¬í•˜ëŠ” computer vision datasetsë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•´ì„œ. / ocrê°™ì€ spanning taskë‚˜, ë¹„ë””ì˜¤ ì† ì•¡ì…˜ ì¸ì‹ì´ë‚˜, geo-localizationì´ë‚˜, ë§ì€ íƒ€ì…ì˜ fine-grained object classificationì´ë‚˜.

   ìš°ë¦¬ëŠ” 30ê°œ ì´ìƒì˜ ë‹¤ë¥¸ computer vision datasetsë¥¼ ë²¤ì¹˜ë§ˆí‚¹ í•´ì„œ ì´ëŸ¬í•œ ì ‘ê·¼ì˜ ì„±ëŠ¥ì„ ì—°êµ¬í–ˆë‹¤. / ocrê°™ì€ spanning taskë‚˜, ë¹„ë””ì˜¤ ì† ì•¡ì…˜ ì¸ì‹ì´ë‚˜, geo-localizationì´ë‚˜, ë§ì€ íƒ€ì…ì˜ fine-grained object classificationì´ë‚˜...

   - SOTA ì°ì€ ê±° ë³´ë©´ í‰ê°€ë„ ì˜ ë‚˜ì™”ê² ì£  ë­.

   - í‰ê°€ëŠ” ì–´ë ¨íˆ ì˜ í•˜ì§€ ì•Šì•˜ì„ê¹Œ

   > The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training.

   > ëª¨ë¸ì€ transferí•œë‹¤. non trivially to most taskê°€.. and modelì€ ì£¼ë¡œ competitiveí•œë‹¤. with a fully supervised baseline without the need for any dataset specific training

   > ëª¨ë¸ì€ transferí•œë‹¤. ì‚¬ì†Œí•˜ì§€ ì•Šì€ ì£¼ìš” taskì—ì„œ. and ëª¨ë¸ì€ ì£¼ë¡œ ì™„ì „í•œ supervised ê¸°ì¤€ìœ¼ë¡œ ê²½ìŸí•œë‹¤. ì–´ë– í•œ datasetì— íŠ¹ì •í•œ trainingì´ í•„ìš”í•˜ì§€ ì•Šë„ë¡.

   ëª¨ë¸ì€ ì‚¬ì†Œí•˜ì§€ ì•Šì€ ì£¼ìš” taskì—ì„œ transferí•œë‹¤. ê·¸ë¦¬ê³ , ëª¨ë¸ì€ ì£¼ë¡œ ì–´ë– í•œ ë°ì´í„°ì…‹ì— íŠ¹ì •í•œ trainingì´ í•„ìš”í•˜ì§€ ì•Šë„ë¡ ì™„ì „í•œ supervise ê¸°ì¤€ìœ¼ë¡œ ê²½ìŸí•œë‹¤.

   - ëª¨ë¸ì´ pre-trainedë§Œ í–ˆìŒì—ë„, ì¦‰, íŠ¹ì • taskì— ëŒ€í•œ ë³„ë„ì˜ trainì´ ì—†ì–´ë„ supervised ëª¨ë¸ê³¼ ê²½ìŸí• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì„.

   - 3ì¤„ìš”ì•½

     - ìš°ë¦¬ ëª¨ë¸ zero-shot ì©”ì–´ìš”.
     - ìš°ë¦¬ ëª¨ë¸ pre-trainingë§Œ í•´ë„ ì„±ëŠ¥ ì˜ë‚˜ì™€ìš”.
     - ìš°ë¦¬ ëª¨ë¸ ì–´ë”°ê°€ ë¶™ì´ë“  ì˜ ì‘ë™í•´ìš”.

   > trainingê³¼ learning ì°¨ì´?<br>
   > train: ëª¨ë¸ íŒŒë¼ë¯¸í„° ìµœì í™”<br>
   > learn: "ë¨¼ì €, í•™ìŠµì€ ì§€ë„ í•™ìŠµ, ë¹„ì§€ë„ í•™ìŠµ, ê°•í™” í•™ìŠµê³¼ ê°™ì´ ëª¨ë¸ì´ ë°ì´í„°ë‚˜ ê²½í—˜ì„ í†µí•´ ì§€ì‹ì´ë‚˜ ìŠ¤í‚¬ì„ íšë“í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤." ... "ë‹¤ì‹œë§í•´ í•™ìŠµì˜ ì¼ë¶€ë¡œì„œ í›ˆë ¨ì´ ì´ë£¨ì–´ì§€ë©°, í›ˆë ¨ì„ í†µí•´ ëª¨ë¸ì´ ë°ì´í„°ì— ëŒ€í•œ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤." [referenced here](https://byunghyun23.tistory.com/152)<br><br>
   > ì„¤ëª…ì´ êµ‰ì¥íˆ ì¶”ìƒì ì´ë¼ë¶€ë‚œ ë­”ê°€ ë” ì´ìƒí•´ì ¸ì‹ ë””..

   > For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.

   > ì˜ˆë¥¼ë“¤ì–´, ìš°ë¦¬ëŠ” ~~~ì˜ ì •í™•ì„±ì„ matchí•œë‹¤. without needing to use any of ~~training examples. it was trained on.

   ì˜ˆë¥¼ë“¤ì–´, ìš°ë¦¬ëŠ” ImageNet zero-shotì— ì›ë˜ ResNet-50ì˜ accuracyë¥¼ matchí•œë‹¤. / 1.28 millionì˜ training examplesë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„

   - í•œì¤„ìš”ì•½
     - ìš°ë¦¬ ëª¨ë¸ ì§±ì§±ì¢‹ìŒ

    <br>

   ë­ì•¼ abstract í•˜ëŠ”ë°ë§Œ 1ì‹œê°„ ë„˜ê²Œ ê±¸ë¦¬ë„¤... ëª¨ë¸ ì–´ì¼€ êµ´ëŸ¬ê°€ëŠ”ì§€ ë‚˜ì˜¤ë©´ í•œ í˜ì´ì§€ í•˜ëŠ”ë° í•œë‚˜ì ˆì€ ê±¸ë¦´ë“¯....ğŸ˜¢

   ë…¼ë¬¸ ë¦¬ë·° ìƒê°ë³´ë‹¤ ë§ì´ë§ì´ í˜ë“¤êµ¬ë‚˜

---

1.  Introduction and Motivating Work

    - pre-training methods(raw textë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ)ë“¤ì´ NLP ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ë³€í™”ë¥¼ ì´ëŒì–´ëƒ„. ìœ„ì—ì„œ ë§í•œ pre-trainingì˜ ì¥ì  ë‹¤ì‹œ ê°•ì¡°.<br>ì•„ë˜ì— gpt3ê°€ ì—¥ê°„í•œ supervised ëª¨ë¸ë“¤ì´ë‘ ê²½ìŸ ê°€ëŠ¥í•œë‹¤ê³  í•˜ë©´ì„œ ì´ë¥¼ ê³„ì† ê°•ì¡°í•¨.

    - autoregressiveì™€ ê°™ì€ Task-agnostic objectives(ë¹„ì¢…ì† ì‘ì—… ëª©í‘œ?)ëŠ” ëª¨ë¸ ìš©ëŸ‰, í•„ìš” ì»´í“¨íŒ… ìì› ë“±ì„ ëŠ˜ë¦¬ë©´ì„œ ì„±ëŠ¥ì„ í–¥ìƒ.

      > task-agnostic objectives: íŠ¹ì • taskì— í•œì •ë˜ì§€ ì•ŠëŠ” ëª¨ë¸<br>
      > autoregressive: ìê¸° íšŒê·€ ëª¨ë¸ë§

    - text2textëŠ” ì…ì¶œë ¥ ì¸í„°í˜ì´ìŠ¤?ë¥¼ í…ìŠ¤íŠ¸ë¡œ standardizedí•¨. -> task-agnostic objectivesê°€ downstream datasetìœ¼ë¡œ zero-shot transfer í•  ìˆ˜ ìˆë„ë¡ í•¨.

      > input-output interface: ?<br>
      > downstream dataset: fine tuningì„ ìœ„í•œ dataset. <br>downstream == fine tuningìœ¼ë¡œ ë´ë„ ë¬´ë°©í• ë“¯.

    - nlpìª½ì—ì„  datasetì„ ë§Œë“¤ ë•Œì—ëŠ” ê¸°ì¡´ high-qualityì˜ crowed-datasetsë³´ë‹¤ web-scale collectionsì„ ì‚¬ìš©í•œ pre-trainingì´ ë” ì¢‹ì•˜ë‹¤ê³  í•¨. -> nlpì²˜ëŸ¼ ì»´í“¨í„° ë¹„ì „ë„ ê¸°ì¡´ crowd labeled dataset ì‚¬ìš©í•˜ì§€ ë§ê³ , ì›¹ì—ì„œ ì‹¹ ê¸ì–´ì˜¤ëŠ” ê±´ ì–´ë–»ëƒê³  ì œì•ˆ.

    1.  `The olfactory bulb: coding and processing of odor molecule information` Mori et al. (1999)

        > Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images.

        - text documentì™€ imageë¥¼ pair í•´ì„œ ë°ì´í„°ë¡œ ì‚¬ìš© -> ëª¨ë¸ì´ ëª…ì‚¬ë‘ í˜•ìš©ì‚¬ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ content based imgae retrievalì„ improving í•˜ë ¤ í•¨.

          > Content-Based Image Retrieval (CBIR): dbì— ì´ë¯¸ì§€ë“¤ì˜ feature vectorë“¤ì„ ì €ì¥, ìƒˆë¡œìš´ ì´ë¯¸ì§€ì˜ feature vectorë¥¼ ë½‘ê³ , ìœ ì‚¬ë„ ê°€ì¥ ë†’ì€ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜í•˜ëŠ” ì¹œêµ¬<br>
          > Retrieval: íŠ¹ì • ê¸°ì¤€ì— ë§ëŠ” ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•˜ê³  ê°€ì ¸ì˜¤ëŠ” ê³¼ì • ì „ì²´?

        <br>

    2.  `Multi-attention Recurrent Network for Human Communication Comprehension` Quattoni et al., 2007

        > Quattoni et al. (2007) demonstrated it was possible to learn more data efficient image representations via manifold learning in the weight space of classifiers trained to predict words in captions associated with images.

        <br>

    3.  `Multimodal Learning with Deep Boltzmann Machines` Srivastava & Salakhutdinov, 2012

        > Srivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features.

        <br>

    4.  `Bag of Tricks for Efficient Text Classification` Joulin et al., 2016

        > Joulin et al. (2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations.

        - ì—°êµ¬ íë¦„ì„ í˜„ëŒ€í™”?í•´ì„œ image cpationsì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê²Œ trainëœ CNNì´ ìœ ìš©í•œ? ì´ë¯¸ì§€ í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ í™•ì¸

          > modernized: ? <br>
          > useful: ?

    5.  6.  ...

    6.  CLIP

        - CLIPì€ ì´ëŸ° ì—°êµ¬ íë¦„?ì˜ ìµœì‹  ê²°ê³¼ë¡œ, ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ íë¦„ì˜ ìµœì‹  ê²°ê³¼ë¬¼ë¡œ, ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ pairë¡œ í•™ìŠµëœ í‘œí˜„ì´ ë‹¤ì–‘í•œ transfer learning taskì—ì„œ ìƒë‹¹í•œ ì„±ëŠ¥ì´ ë‚˜ì˜´ì„ ë³´ì„.

    <br>

    > While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches. For example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012). Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance. Mahajan et al. (2018) showed that predicting ImageNet-related hashtags on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time. Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset.

    - natural language supervisionìœ¼ë¡œ image representation learningì„ í•˜ëŠ” ê²ƒ ìì²´ëŠ” exciting as proofs of conceptí•˜ëŠ”ë°, ì‹¤ì œë¡œ ë²¤ì¹˜ ëŒë ¤ë³´ë©´ alternative approachesë³´ë‹¤ ì•„ì‰¬ìš´ ì„±ëŠ¥ì„ ëƒ„.

      > zero-shot ImageNet ì •í™•ë„
      >
      > - ìì—°ì–´ supervision Li et al. (2017): 11.5%
      > - ìµœì‹  ì—°êµ¬ Xie et al. (2020): 88.4%
      > - ê¸°ì¡´ ì—°êµ¬ Deng et al. (2012): 50%

      > natural language supervision: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ë¡œ image classifierë¥¼ trian

    - ê·¼ë°, weak supervisionì„ ì‚¬ìš©í•˜ë‹ˆê¹Œ ì„±ëŠ¥ì´ 5% ì´ìƒ ì¢‹ì•„ì§. ì—¬ê¸°ì„œ ë§í•˜ëŠ” weak supervisionì€ pre-trained modelì„ íŒŒì¸íŠœë‹ í•˜ëŠ” ê±° ë§í•˜ëŠ”ë“¯

      > weak supervision: ì¢ì€ ë²”ìœ„ì˜ ëª…í™•í•œ ëª©í‘œ -

    - Kolesnikov et al. (2019)ì´ë‘ Dosovitskiy et al. (2020)ì—ì„œ ë…¸ì´ì¦ˆ ê°€ë“ê°€ë“í•œ ë¼ë²¨ì„ ì˜ˆì¸¡í•˜ë„ë¡ pre-trainingí•¨. -> ë²¤ì¹˜ì—ì„œ ìƒë‹¹íˆ ì˜ ë‚˜ì˜´.

    > This line of work represents the current pragmatic middle ground between learning from a limited amount of supervised â€œgold-labelsâ€ and learning from practically unlimited amounts of raw text. However, it is not without compromises. Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their â€œzero-shotâ€ capabilities.

    - ìœ„ 2ê°œ ì—°êµ¬ëŠ” ì–‘ì´ limitedëœ gold-labelì´ë‘ unlimitedëœ raw text ì‚¬ì´ì—ì„œì˜ ì¤‘ê°„ ì§€ì ì„ ë‚˜íƒ€ëƒ„.

      > gold-label: ì‚¬ëŒì´ ì§ì ‘ ë¼ë²¨ë§í•œ ë¼ë²¨ë“¤... <br>ì¡°ë‹ˆê³¨ë“œ ë•¡ê¸°ë„¤ 18ë…„ ë§ê³  ë¦¬ì €ë¸Œë¡œ.. ë§›ì€ ì–´ì‹ ë”” ê·¸ ìŠ¤ëª¨í‚¤í•œ íŒ”ë ˆíŠ¸ê°€ ë•¡ê¹€

    - ë‘ ì—°êµ¬ ë‹¤ supervisionì„ ê°ê° 1000ê°œì™€ 18291ê°œì˜ í´ë˜ìŠ¤ë¡œ ì œí•œí•˜ë„ë¡ ì„¤ê³„í•¨.

      \+ ìì—°ì–´ëŠ” ë” ì¼ë°˜ì ì´ë¼ì„œ gold-labelë³´ë‹¤ ë” ë„“ì€ ë²”ìœ„ì˜ ì‹œê°ì  ê°œë…ì„ í‘œí˜„ & superviseí•  ìˆ˜ ìˆìŒ.

    - ë‘˜ ë‹¤ static softmax classifierë¥¼ ì‚¬ìš©í•´ì„œ ì˜ˆì¸¡ -> dynamic-ouputì´ ë¶€ì¡±..? -> flexibilityë‘ zero-shotì˜ í•œê³„

    <br>

    > ã…‡ã…‡ã„¹ã…‡ã„¹ã…‡ã„¹

    - weak supervision: Mahajan et al. (2018), Kolesnikov et al. (2019)ì€ ë°±ë§Œ~ì‹­ì–µ ê°œì˜ ì´ë¯¸ì§€ë¡œ ëª¨ë¸ì„ trainí•˜ëŠ”ë° accelerator yearsê°€ ë¨..?

      > accelerator years: acceleratorë¡œ ìˆ˜ë…„ì„ ëŒë ¤ì•¼ í•œë‹¤ ì´ëŸ° ëœ»ì¸ê°€

    - ìì—°ì–´ supervision: VirTex, ICMLM, ConVIRT ë“±ì€ ì‹­ë§Œ ê°œì˜ ì´ë¯¸ì§€ë¥¼ trainí•˜ëŠ”ë° accelerator daysê°€ ë¨.

    > In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision.

    - ì´ work?ì—ì„  this gap?ì„ ê°€ê¹ê²Œ í•˜ê³ , large scaleì˜ natural language supervisionìœ¼ë¡œ trainedëœ image classifierë¥¼ ì—°êµ¬

      > ì—¬ê¸°ì„œ ë§í•˜ëŠ” `this gap`ì´ train ë°ì´í„°ë¥¼ ë§í•˜ëŠ” ê±´ê°€..<br>
      > close this gap: weak supervisionê³¼ì˜ scale ì°¨ì´ë¥¼ ì¤„ì¸ë‹¤ -> ìì—°ì–´ supervisionì˜ train scaleì„ ëŠ˜ë¦°ë‹¤ ì´ë ‡ê²Œ í•´ì„í•˜ë©´ ë ë“¯

    - ì¸í„°ë„·ì—ì„œ 400 millionì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ê¸ì–´ì˜´

    - CLIP(Contrastive Language-Image Pre-training)(ë‹¨ìˆœí™”ëœ ConVIRT)ì€ íš¨ìœ¨ì ì¸ learning method from natural language supervision

    > We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and observe that transfer performance is a smoothly predictable function of compute

    - CLIP ëª¨ë¸ì˜ scalabilityë¥¼ ì—°êµ¬ -> 2ë°°ì˜ ì»´í“¨íŒ… í¬ê¸°ì˜ 8ê°€ì§€ ëª¨ë¸ì„ í›ˆë ¨í•´ì„œ transfer performanceê°€ smoothly predictable function?ì¸ì§€

      - transfer performanceê°€ í¬ê¸°ì— ë”°ë¼ ë¶€ë“œëŸ½ê²Œ ì˜ˆì¸¡ ê°€ëŠ¥í•œ í•¨ìˆ˜ì˜ í˜•íƒœë¡œ ë³€í•¨ -> ì»´í“¨íŒ… í¬ê¸° ì¦ê°€ì— ë”°ë¼ ì„±ëŠ¥ì´ ì¼ì •í•˜ê²Œ ì¦ê°€

      - ìì›ì„ ë§ì´ ë„£ì„ìˆ˜ë¡ ì„±ëŠ¥ ì¦ê°€

        > ë­ì•¼ ë„ˆë¬´ ë‹¹ì—°í•œ ì´ì•¼ê¸°ì¸ë° ë‹¹ì—°í•˜ì§€ ì•Šë“¯ì´ ë§í•˜ë‹ˆê¹Œ ë­ì§€ ì‹¶ì—ˆë„¤

    > We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others.

    - CLIPë„ GPTì²˜ëŸ¼ pre-trainingì„ í†µí•´ OCR, geo-localization ë“±ë“± ë‹¤ì–‘í•œ ê±° í•  ìˆ˜ ìˆìŒ.

    > We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find it can be competitive with prior task-specific supervised models.

    - 30ê°œê°€ ë„˜ëŠ” ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ CLIPì˜ zero-shot transfer performanceë¥¼ ë²¤ì¹˜ë§ˆí‚¹ -> competitive with prior task-specific supervised models

    > We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient

    - linear-probe representation learningì„ ë¶„ì„í•˜ë‹ˆ ImageNet model ì¤‘ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ê³ , íš¨ìœ¨ë„ ê´œì°®ì•˜ìŒ.

      > linear-probe: íŠ¹ì • ëª¨ë¸ì´ í•™ìŠµí•œ representationì„ í‰ê°€í•˜ëŠ” ë°©ë²•. ëª¨ë¸ì´ í•™ìŠµí•œ í‘œí˜„ì„ ê³ ì •í•˜ê³  ê·¸ ìœ„ì— linear regressionì„ í•™ìŠµì‹œì¼œì„œ í‰ê°€

    > We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a modelâ€™s capability. These results have significant policy and ethical implications, which we consider in Section 7.

    - zero-shot CLIPì´ ë¹„ìŠ·í•œ ì •í™•ë„ì˜ ë‹¤ë¥¸ supervised ëª¨ë¸ë³´ë‹¤ robust?í•¨. -> zero-shot evaluationì´ task-agnostic ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ ë” ì˜ ëŒ€í‘œí•¨. -> íŠ¹ì • ì‘ì—… ë¿ë§Œì´ ì•„ë‹Œ, ë²”ìš©ì„± ë†’ìŒ.

      > robustness: ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ëŠ¥ë ¥. -> ë°ì´í„° ë³€í™” or ë…¸ì´ì¦ˆ ë“±ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ìœ ì§€í•˜ëŠ”ì§€

      \+ section 7ì—ì„œ ì •ì±…, ìœ¤ë¦¬ì ì¸ ì‹œì‚¬ì ? ê´€ë ¨í•´ì„œë„ ì´ì•¼ê¸° í•¨

---

## 2. Approach

### 2.1. Natural Language Supervision

> At the core of our approach is the idea of learning perception from supervision contained in natural language

- key idea: ìì—°ì–´ì— í¬í•¨ëœ supervisionì„ í†µí•´ perceptionì„ learningí•˜ëŠ” ê²ƒ.

> As
> discussed in the introduction, this is not at all a new idea, however terminology used to describe work in this space is varied, even seemingly contradictory, and stated motivations are diverse.
>
> Joulin et al. (2016), and Desai & Johnson (2020) all introduce methods which learn visual representations from text paired with images but describe their approaches as unsupervised, self-supervised, weakly supervised, and supervised respectively.

- ì™„ì „íˆ ìƒˆ ì•„ì´ë””ì–´ëŠ” ì•„ë‹˜, ê·¼ë° ì œëŒ€ë¡œ ì •ì˜ë˜ì§€ ì•Šì€ ë¶€ë¶„ì´ ë§ìŒ -> Joulin et al. (2016), and Desai & Johnson (2020)ëŠ” ì´ë¯¸ì§€ì™€ pairëœ í…ìŠ¤íŠ¸ë¡œ í‘œí˜„í•¨. but ì–˜ë„¤ëŠ” ê°ê° approachesëŠ” unspervised, self-supervised, weakly supervised, supervisedë¡œ ì ‘ê·¼í–ˆìŒ

  - ê·¼ë° ì´ê²Œ ì™œ..? ë­ê°€ ë¬¸ì œì•¼

> We emphasize that what is common across this line of work is not any of the details of the particular methods used but the appreciation of natural language as a training signal.
>
> All these approaches are learning from natural language supervision.

- ì—¬íŠ¼ ë‹¤ì–‘í•œ ë°©ì‹ì´ ì‚¬ìš©ëì§€ë§Œ, ëª¨ë‘ `ìì—°ì–´ë¥¼ training signal`ë¡œ ì¸ì‹. -> ìì—°ì–´ë¥¼ í†µí•´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ì ‘ê·¼ ë°©ì‹ì´ ì£¼ìš” í‚¤í¬ì¸íŠ¸.

> Although early work wrestled with the complexity of natural language when using topic model and n-gram representations, improvements in deep contextual representation learning suggest we now have the tools to effectively leverage this abundant source of supervision (McCann et al., 2017).

ë­ì•¼ì™¤ì¼€ê¸¸ì–´ìš”ì˜ì–´ê³µë¶€ì—´ì‹¬íˆí•´ë†“ì„ê±¸

- early workë“¤ì€ ìì—°ì–´ì˜ complexityë¡œ ì–´ë ¤ì›€ì„ ê²ªìŒ / topic model & n-gram representationsì„ ì‚¬ìš©í•  ë•Œì—..

  > early work: ì´ˆê¸° ì—°êµ¬
  > topic model: ë¬¸ì„œ ë‚´ì˜ topicì„ ì¶”ì¶œí•˜ëŠ” ê¸°ë²•<br>
  > n-gram: ì—°ì†ëœ nê°œì˜ ë‹¨ì–´ë¥¼ ë¬¶ì–´ì„œ language modelì„ ë§Œë‹¤ëŠ” ë°©ë²•

- deep contextual representation learningì—ì„œì˜ ë°œì „ -> abundant source of supervisionì„ ì œëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ê°€ ìƒê¹€.

  - lstn? transformer? ë„êµ¬ëŠ” ì´ëŸ° ì• ë“¤ ë§í•˜ëŠ” ê±´ê°€?
  - ì•„ë‹ˆë©´ bertë‚˜ gptê°™ì€ ëª¨ë¸ë“¤ì„ ë§í•˜ëŠ” ê±´ê°€

  > natural languageëŠ” ë§ ê·¸ëŒ€ë¡œ ë‚´í¬í•˜ëŠ” ë°ì´í„° ìì²´ê°€ í’ë¶€í•´ì„œ ì œëŒ€ë¡œ ì¨ë¨¹ì„ ìˆ˜ë§Œ ìˆë‹¤ë©´ training dataë¡œ ìœ ìš©í•˜ê²Œ ì“°ì¼ ìˆ˜ ìˆìŒ -> ì´ê±¸ ì œëŒ€ë¡œ í•  ìˆ˜ ìˆê²Œë˜ì—ˆë‹¤ëŠ” ê±¸ ë§í•˜ëŠ”ë“¯

> Learning from natural language has several potential strengths over other training methods. Itâ€™s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification since it does not require annotations to be in a classic â€œmachine learning compatible formatâ€ such as the canonical 1-of-N majority vote â€œgold labelâ€. Instead, methods which work on natural language can learn passively from the supervision contained in the vast amount of text on the internet. Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesnâ€™t â€œjustâ€ learn a representation but also connects that representation to language which enables flexible zero-shot transfer. In the following subsections, we detail the specific approach we settled on.

- ìì—°ì–´ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒ -> scability ë›°ì–´ë‚¨, ì¶”ê°€ì ì¸ labeling í•„ìš” ì—†ìŒ, ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš© ê°€ëŠ¥ ...

  - clipì€ ì´ë ‡ê²Œ ìì—°ì–´-ì´ë¯¸ì§€ í•¨ê»˜ í•™ìŠµí•´ì„œ zero-shot ë›°ì–´ë‚¨...

  > ì´ê±¸ ê³„ì†ê³„ì† ê°•ì¡°í•˜ë„¤

### 2.2. Creating a Sufficiently Large Dataset

1. ê¸°ì¡´ ë°ì´í„°ì…‹ì˜ í•œê³„

   - MS-COCO, Visual Genome: high quality crowd-labeled, 10ë§Œ ì¥ì˜ ì´ë¯¸ì§€
     - ë‹¤ë¥¸ computer vision systemsì€ ì¸ìŠ¤íƒ€ì—ì„œ ê°€ì ¸ì˜¨ 3.5 billion ì¥ì˜ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•¨ ->
   - YFCC100M: 100 million ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì„œ ê·¸ë‚˜ë§ˆ ëŒ€ì•ˆì´ ë  ìˆ˜ ìˆìŒ but qualityê°€ varyingí•˜ê³ , metadataê°€ sparseí•¨.
     - ì¹´ë©”ë¼ ì„¤ì •ê°’, ì˜ë¯¸ ì—†ëŠ” ì œëª© ë“±ì„ ê±°ë¥´ë‹ˆ 15 million ì¥ì˜ ì´ë¯¸ì§€ë§Œ ë‚¨ìŒ.

2. natural language supervisionë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ 

   - ì¸í„°ë„·ì—ëŠ” large quantities of dataê°€ ìˆìŒ. but ê¸°ì¡´ datasetì€ ì´ë¥¼ ì˜ ë°˜ì˜í•˜ì§€ ëª»í•¨.

   - ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ 400 millionì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì„ ì¸í„°ë„·ì—ì„œ êµ¬í•¨. + 500,000ì˜ ì¿¼ë¦¬ë„ í¬í•¨.

   - class balanceë¥¼ ìœ„í•´ ì¿¼ë¦¬ë‹¹ ìµœëŒ€ 20,000ì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì„ í¬í•¨.

   - datasetì˜ ë‹¨ì–´ ìˆ˜ëŠ” gpt2ë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ ì‚¬ìš©í•œ WebText datasetê³¼ ë¹„ìŠ·.

   - We refer to this dataset as WIT for WebImageText.

### 2.3. Selecting an Efficient Pre-Training Method

open set of visual concepts

- ìµœì‹  ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œì˜ ê³„ì‚° ìì›:
  ResNeXt101-32x48dë¥¼ trainí•˜ëŠ”ë° 19 GPU years ì‚¬ìš©, Noisy Student EfficientNet-L2ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ”ë° 33 TPUv3 core-years ì‚¬ìš©.. -> ëŒ€ì¶© ìµœê·¼ ë‚˜ì˜¤ëŠ” ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œ í›ˆë ¨ì‹œí‚¤ëŠ”ë° êµ‰ì¥íˆ ì˜¤ë˜ ê±¸ë¦¼.

  - ì–˜ë„¤ë“¤ì€ ImageNet datasetì— ìˆëŠ” 1000ê°œì˜ í´ë˜ìŠ¤ë§Œì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ í›ˆë ¨ë¨.

- ìœ„ ëª¨ë¸ë“¤ì€ 1000ê°œì˜ í´ë˜ìŠ¤ë§Œì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë¨.

- clipì€ ìì—°ì–´ë¡œ open set of visual concepts í•™ìŠµí•˜ëŠ” ê²ƒì´ ì–´ë µê¸´ í•œë° ì–´ì°Œì €ì°Œí•´ì„œ ë°©ë²• ì°¾ì•„ëƒ„.

  - natural language supervisionì„ successfully scalingí•˜ê¸° ìœ„í•´ì„œëŠ” training efficiencyê°€ keyë¼ëŠ” ê²ƒì„ ì°¾ìŒ.

- ì´ˆê¸°ì—” VirTexë‘ ë¹„ìŠ·í•˜ê²Œ image CNN - text transformerìœ¼ë¡œ ì´ë¯¸ì§€ captionì„ ì˜ˆì¸¡í•˜ë„ë¡ í•¨. but ì´ ë°©ë²•ì€ efficiently scalingí•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ë°œìƒ.

  > difficulties efficiently scaling: ë” ë§ì€ ë°ì´í„°ë‚˜ ë” í° ëª¨ë¸ì„ ì‚¬ìš©í•´ë„ ì„±ëŠ¥ í–¥ìƒì´ ë”ë”˜ -

  ![figure2](figure2.png)

  - 63 million íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ transformer language modelì´ ResNet-50 image encoderë³´ë‹¤ computeë¥¼ ë°°ë¡œ í•˜ì§€ë§Œ, ImageNet í´ë˜ìŠ¤ë¥¼ recognizeí•˜ëŠ”ë° 3ë°°ê°€ ëŠë¦¼.

  -> ë³µì¡í•œ transformer language modelì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì˜¤íˆë ¤ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ + ë‹¨ìˆœí•œ ëª¨ë¸ì´ ë” ë¹ ë¥´ê³ , ê°€ë²¼ìš´ë° ì„±ëŠ¥ì€ ë¹„ìŠ·.

  - cuz ì´ëŸ° ë°©ë²•ë“¤ì€ try to predict the exact words of the text accompanying each imageí•˜ë ¤ í•˜ê¸° ë•Œë¬¸. ì´ë¯¸ì§€ì™€ co-occurí•˜ëŠ” descriptions, comments, and related textëŠ” ë‹¤ì–‘í•´ì„œ, ì •í™•í•œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ì–´ë ¤ì›€.

    -> ê·¸ë˜ì„œ Contrastive Learning ì‚¬ìš©.

  - ìµœê·¼ ì—°êµ¬ì—ì„œ, ì´ë¯¸ì§€ì— ëŒ€í•œ contrastive representation learningì´ contrastive objectivesë³´ë‹¤ ë” ì¢‹ì€ representationì„ learní•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì„. + generative models of imagesì€ high quality image representationì„ learn í•  ìˆ˜ ìˆì§€ë§Œ, they require over an order of magnitude more compute than contrastive models with the same performance.

    > contrastive model: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ pairë¥¼ í•™ìŠµ -> ìœ ì‚¬ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” -

    -> ê·¸ë˜ì„œ clip ëª¨ë¸ ë§Œë“  ì¹œêµ¬ë“¤ì€ textì˜ exact wordsë¥¼ predictí•˜ëŠ” ëŒ€ì‹ , text ì „ì²´ê°€ ì–´ë– í•œ ì´ë¯¸ì§€ì™€ pairedë˜ì—ˆëŠ”ì§€ë§Œ predictí•˜ë„ë¡ - / `explored training a system to solve the potentially easier proxy task` í•¨

    > ì—¬ê¸°ì„œ ë§í•˜ëŠ” easier proxy taskê°€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ìŒì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹?

  - ì•”íŠ¼ contrastive model ì‚¬ìš©í•´ì„œ ê¸°ë³¸ì ì¸ Bag-of-Words encodingì„ swapped í•˜ê³ , & observed a further 4x efficiency improvement in the rate of zero-shot transfer to ImageNet.

  2. simplification part

     - Nê°œì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì˜ batch
     - goal

       - batch ë‚´ì—ì„œ ê°€ëŠ¥í•œ N \* N ê°œì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì¡°í•© ì¤‘, ì‹¤ì œë¡œ ì¼ì–´ë‚œ Nê°œì˜ ìŒì„ ì˜ˆì¸¡.
       - image encoderì™€ text encoderë¥¼ ì‚¬ìš©í•´ì„œ batchì—ì„œ Nê°œì˜ ì‹¤ì œ pairëœ image embedingê³¼ text embedingì˜ (Nê°œ) cosine simmilarityëŠ” maximizing, ê·¸ ì™¸ì— embedingë“¤(N^2 - Nê°œ)ì—ì„œëŠ” minimizingì´ ë˜ë„ë¡.
       - symmetric cross entropy lossë¡œ similarity scoresë¥¼ ìµœì í™”.

     - ë°ì´í„° ì–‘ì´ ì›Œë‚™ ë§ì•„ì„œ over fittingë¬¸ì œëŠ” ì—†ìŒ.

     - ImageNet weightsë‚˜ pre-trained weightsë¡œ initializing í•˜ëŠ” ê²ƒ ì—†ì´ train -> ì™„ì „ zero-shotì— íŠ¹í™”ëœ -

     - do not use the non-linear projection between the representation and the contrastive embedding space -> linear projectionë§Œì„ ì‚¬ìš©í•´ì„œ ê°ê°ì˜ encoderì˜ representationì„ multi-modal embedding spaceìœ¼ë¡œ mapping.

       > representation: encoderê°€ input dataë¥¼ ì²˜ë¦¬í•´ì„œ ìƒì„±í•˜ëŠ” high-demention vector
       >
       > projection: input dataë¥¼ lowe-dimension vectorë¡œ ë³€í™˜ -> íŠ¹ì§•ë§Œ ë½‘ì•„ëƒ„. cnnì˜ feature vectorì²˜ëŸ¼ ì´í•´í•˜ë©´ ë ë“¯.
       >
       > contrastive embedding space: contrastive learningì˜ embedding vector. ë¹„ìŠ·í•œ ë°ì´í„°ëŠ” ê°€ê¹Œìš´ vectorë¡œ í‘œí˜„í•˜ëŠ”
       >
       > multi-modal embedding space: ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ê°™ì€ ê³µê°„ìœ¼ë¡œ mapping
       >
       > mapping: í•œ ì§‘í•©ì˜ ìš”ì†Œë¥¼ ë‹¤ë¥¸ ì§‘í•©ì˜ ìš”ì†Œë¡œ ë³€í™˜

       > We did not notice a difference in training efficiency between the two versions and speculate that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods.
       >
       > - linearì™€ non-linear ì‚¬ì´ì— training efficiency ì°¨ì´ëŠ” ì–´ì‹ ë””, non-linear projectionì€ self-supervised representation learning methodsì—ì„œë§Œ current imageì˜ detailsì™€ co-adaptedë  ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤... ëŠ” ë­” ë§ì´ì§€

     - clipì˜ pre-train dataset(img-txt pair)ì˜ ëŒ€ë¶€ë¶„ì˜ sentenceê°€ single sentenceì´ê¸°ì— Zhang et al. (2020)ì—ì„œ ì‚¬ìš©ëœ text transformation functionì„ ì œê±°.

       > text transformation function: samples a single sentence at uniform from the text
       >
       > ì–˜ë¥¼ ì§€ìš°ëŠ” ê²Œ ë­” ì˜ë¯¸ê°€ ìˆëŠ” ê±°ì§€..? ì´ë¯¸ single sentenceê¸°ì— êµ³ì´ ì´ funcê°€ í•„ìš” ì—†ë‹¤ëŠ” ê±´ê°€

     - image transformation functionë¥¼ ë‹¨ìˆœí™”, A random square crop from resized imagesê°€ training ì¤‘ì— ì‚¬ìš©ëœ ìœ ì¼í•œ data augmentation.

       > ì•ˆê·¸ë˜ë„ ë°ì´í„° ë§ì€ë° êµ³ì´ ë³µì¡í•˜ê²Œ í•´ì„œ ì¼ ìƒê¸¸ ê°€ëŠ¥ì„±ì„ ë§Œë“¤ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•´?

     - softmaxì—ì„œ range of the logitsë¥¼ controlsí•˜ëŠ” temperature parameterì¸ Ï„ëŠ” hyper-parameterë¡œ turningí•˜ëŠ” ê²ƒì„ avoidí•˜ê¸° ìœ„í•´ log-parameterizedëœ multiplicative scalarë¥¼ training ì¤‘ì— optimized.

     -> datasetì´ ì—¥ê°„íˆ í° ê²Œ ì•„ë‹ˆë‹ˆê¹Œ over fitting ë¬¸ì œx -> ë³µì¡í•œ ê¸°ë²•ë“¤ì„ ê°„ì†Œí™”

---

### 2.4. Choosing and Scaling a Model

- img encoderëŠ” ResNet-50ì´ë‘ ViTë¥¼ considerí•¨.

  - `ResNetD` and `antialiased rect-2 blur pooling`ë¥¼ usingí•´ì„œ make several modifications to the original versioní•¨.

    > ResNetD: ResNetì˜ ê°œì„ 
    >
    > antialiased rect-2 blur pooling: img processingì„ ë” ë¶€ë“œëŸ½ê²Œ -> sampling ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì™œê³¡ì„ ì¤„ì„

  - replace the global average pooling layer with an attention pooling mechanism

    - attention poolingì€ queryê°€ the global average-pooled representation of the imageì— ë”°ë¼ conditionedë˜ëŠ” â€œtransformer-styleâ€ì˜ multi-head QKV attentionì˜ single layerë¡œ implemented ë¨... ? ë­”ì†Œë¦¬ì•¼ã€€ì´ê±´

    > global average pooling layer: cnnì—ì„œ ì‚¬ìš©ë˜ëŠ” pooling ê¸°ë²•, GAP layerëŠ” input feature mapì˜ ê° ì±„ë„ë³„ë¡œ í‰ê· ê°’ì„ ê³„ì‚°í•´ì„œ í•˜ë‚˜ì˜ ìˆ«ìë¡œ ì••ì¶•
    >
    > attention pooling mechanism: attentionì„ ì‚¬ìš©í•´ì„œ ì¤‘ìš”í•œ featureë¥¼ ê°•ì¡°. input dataì˜ íŠ¹ì • ë¶€ë¶„ì— ê°€ì¤‘ì¹˜ë¥¼ ì¤Œ.

  - transformer ì•ì— ê²°í•©ëœ combined patchì™€ position embeddingsì— additional layer normalizationì„ í•˜ê³ , ê¸°ì¡´ ViTì™€ëŠ” ë‹¤ë¥¸ initialization schemeì„ ì‚¬ìš©.

    > ê¸°ì¡´ ë°©ë²•ë“¤ì„ ìˆ˜ì •í•´ì„œ ì„±ëŠ¥ í–¥ìƒ & ìµœì í™” ì§„í–‰

- text encoderëŠ” attention all you needì˜ transformerë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨.

  - Radford et al. (2019).ì´ describedí•œ architecture modificationsê°€ ì ìš©ë¨.

  - 63M-parameter 12-layer 512-wide model with 8 attention headsë¡œ êµ¬ì„±ë¨.

  - í…ìŠ¤íŠ¸ëŠ” Lower-casedë¡œ ë³€í™˜, (BPE)ë¡œ 49,152 vocab sizeë¡œ í† í°í™”ë¨.

    > BPE: í…ìŠ¤íŠ¸ë¥¼ sub-word ë‹¨ìœ„ë¡œ ë¶„í• í•´ì„œ ì²˜ë¦¬

  - For computational efficiency, the max sequence length was capped at 76

  - The text sequenceëŠ” [SOS] and [EOS] tokensë¡œ ë¬¶ì„.

    - Transformerì˜ highest layerì—ì„œ [EOS] token ìœ„ì¹˜ì˜ activationsê°€ feature representation of the text ë¡œ ì²˜ë¦¬..?

    - ì´ëŸ¬í•œ feature representationì€ layer normalization ì´í›„, multi-modal embedding spaceë¡œ linearly projectedë¨.

  - text encoderì—ëŠ” pre-trainedëœ language modelë¡œ initializeí•˜ê±°ë‚˜ add language modeling as an auxiliary objective ê¸°ëŠ¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ masked self-attention ì‚¬ìš© + though exploration of this is left as future work.

    > masking: ê° tokenì´ ë°”ë¡œ ì• tokenë§Œì„ ì°¸ê³ í•˜ë„ë¡ í•¨.

- ê¸°ì¡´ computer vision researchì—ì„œëŠ” ì¢…ì¢… width (Mahajan et al., 2018) or depth (He et al., 2016a)ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ ëª¨ë¸ì„ scaledí•¨. but ResNet image encoderì—ì„  width, depth, and resolution ëª¨ë‘ì— allocating additional computeí•˜ëŠ” ê²ƒì´ only allocating it to only one dimension í•˜ëŠ” ê²ƒë³´ë‹¤ outperforms í•˜ë‹¤ëŠ” Tan & Le (2019)ì˜ approachë¥¼ ë”°ë¦„.

- Tan & Le(2019)ëŠ” allocated to each dimensionë˜ëŠ” ratio of computeë¥¼ EfficientNet architectureì— ë§ê²Œ tune í•¨. clip ì—°êµ¬ì—ì„  width, depth, and resolution of modelì„ increasingí•˜ëŠ”ë° additional computeë¥¼ equallyí•˜ê²Œ allocatingí•˜ëŠ” simple baselineì„ ì‚¬ìš©í•¨.

- For the text encoder,

---

- figure1.
  ![alt text](figure1.png)

  > While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target datasetâ€™s classes

  - standard image modelsëŠ” image feature extractorì™€ linear classifierë¥¼ í•¨ê»˜ í•™ìŠµì‹œì¼œ ëª‡ëª‡ ë¼ë²¨ì„ ì˜ˆì¸¡.

  - CLIPì€ image encoderì™€ text encoderë¥¼ í•¨ê»˜ í•™ìŠµì‹œì¼œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ pairì˜ training examples batchì˜ ì •í™•í•œ pairingì„ ì˜ˆì¸¡.

  -> í…ŒìŠ¤íŠ¸ ì‹œ í•™ìŠµëœ test encoderê°€ ëŒ€ìƒ datasetì˜ class nameì´ë‚˜ descriptionì„ í¬í•¨ì‹œì¼œì„œ zero-shot linear classifierë¥¼ í•©ì„±?
